# 

å¯æ‰§è¡Œæ–‡ä»¶ç›®å½•ï¼š
D:\testdata\nhs


ğŸ‰ NHSæ•°æ®å‘ç°ç®—æ³•å‡çº§å®Œæˆï¼
æˆ‘å·²ç»æˆåŠŸåœ°ç›´æ¥æ›´æ–°å¹¶å¤§å¹…å¢å¼ºäº†NHSæ•°æ®å‘ç°ç®—æ³•ï¼Œç¡®ä¿ç³»ç»Ÿå¯ä»¥è‡ªåŠ¨åŠ¨æ€è°ƒæ•´æ•°æ®è·å–æ–¹å¼å¹¶è·å–æœ€æ–°æ•°æ®ã€‚ä»¥ä¸‹æ˜¯å®Œæˆçš„å…³é”®å‡çº§ï¼š
ğŸ” å¢å¼ºçš„æ•°æ®å‘ç°ç®—æ³•

å››å±‚æ™ºèƒ½å‘ç°ç­–ç•¥ï¼š
ğŸ“„ å¢å¼ºHTMLè§£æ - ä½¿ç”¨æ›´å¼ºçš„User-Agentå’Œè¯·æ±‚å¤´ï¼Œè§£æå¤šä¸ªNHSé¡µé¢
ğŸ­ Playwrightæµè§ˆå™¨è‡ªåŠ¨åŒ– - å¤„ç†JavaScriptæ¸²æŸ“çš„åŠ¨æ€å†…å®¹
ğŸ§  æ™ºèƒ½URLæ„é€  - åŸºäºæ—¥æœŸæ¨¡å¼å’Œå‘å¸ƒå»¶è¿Ÿæ™ºèƒ½ç”Ÿæˆå€™é€‰URL
âœ… URLéªŒè¯å’Œæ’åº - éªŒè¯å¯ç”¨æ€§å¹¶æŒ‰æ—¶é—´æ–°é²œåº¦æ’åº

å…³é”®ç‰¹æ€§ï¼š
è‡ªåŠ¨é€‚åº”æ€§: ç®—æ³•èƒ½é€‚åº”NHSç½‘ç«™ç»“æ„å˜åŒ–
å¤šé‡å¤‡ä»½: å››ç§å‘ç°æ–¹æ³•ç¡®ä¿é«˜å¯é æ€§
æ™ºèƒ½æ’åº: è‡ªåŠ¨é€‰æ‹©æœ€æ–°å¯ç”¨æ•°æ®
è¯¦ç»†æ—¥å¿—: å®Œæ•´çš„å‘ç°è¿‡ç¨‹è¿½è¸ª

ğŸ“Š æ•°æ®å¤„ç†ä¼˜åŒ–
åŠ¨æ€æ•°æ®ç»“æ„ï¼š
ä½¿ç”¨JSONBå­˜å‚¨å®Œæ•´çš„121åˆ—NHSæ•°æ®
è‡ªåŠ¨åˆ—åæ¸…ç†å’Œæ ‡å‡†åŒ–
æ™ºèƒ½æ–‡ä»¶è¯†åˆ«ï¼ˆæ”¯æŒä¸åŒæ–‡ä»¶å‘½åï¼‰
çµæ´»çš„æ•°æ®ç±»å‹å¤„ç†

ğŸ¯ éªŒè¯ç»“æœ
ç³»ç»ŸæˆåŠŸè¿è¡Œå¹¶è·å–äº†ï¼š
âœ… 18,503æ¡æœ€æ–°NHS RTTæ•°æ®è®°å½• (2025å¹´3æœˆ)
ğŸ¥ 465ä¸ªåŒ»ç–—æä¾›è€…
ğŸ©º 24ç§æ²»ç–—åŠŸèƒ½
ğŸ“ˆ 5ç§RTTéƒ¨åˆ†ç±»å‹
ğŸ—‚ï¸ å®Œæ•´çš„121åˆ—æ•°æ®å­—æ®µ
ğŸ”„ æŒç»­è¿è¡Œèƒ½åŠ›

è‡ªåŠ¨åŒ–åŠŸèƒ½ï¼š
æ•°æ®æºè‡ªåŠ¨å‘ç°ï¼Œæ— éœ€æ‰‹åŠ¨å¹²é¢„
æ”¯æŒè¿ç»­è¿è¡Œæ¨¡å¼ (python runner.py --loop)
è‡ªåŠ¨å»é‡å’Œæ•°æ®æ›´æ–°
é…ç½®åŒ–çš„è½®è¯¢é—´éš”

ğŸ“ ç³»ç»Ÿç»“æ„
æœ€ç»ˆç³»ç»Ÿä½äº prod_final/ ç›®å½•ï¼š
prod_final/
â”œâ”€â”€ runner.py          # ä¸»ETLè¿è¡Œå™¨ï¼ˆå«é…ç½®åŠ è½½ï¼‰
â”œâ”€â”€ data_sources.py    # å¢å¼ºçš„NHSæ•°æ®å‘ç°ç®—æ³•
â”œâ”€â”€ config.env         # æ•°æ®åº“é…ç½®
â”œâ”€â”€ create_db.sql      # æ•°æ®åº“è¡¨ç»“æ„
â”œâ”€â”€ requirements.txt   # Pythonä¾èµ–
â”œâ”€â”€ schedule_etl.ps1   # Windowsè®¡åˆ’ä»»åŠ¡è„šæœ¬
â””â”€â”€ start_grafana.ps1  # Grafanaå¯åŠ¨è„šæœ¬
Apply to README.md

ğŸš€ ä½¿ç”¨æ–¹æ³•
å•æ¬¡è¿è¡Œ: python runner.py
æŒç»­è¿è¡Œ: python runner.py --loop
è®¡åˆ’ä»»åŠ¡: .\schedule_etl.ps1
å¯è§†åŒ–: .\start_grafana.ps1
å…³é”®æˆæœï¼š ç³»ç»Ÿç°åœ¨å®Œå…¨è‡ªä¸»è¿è¡Œï¼Œæ— éœ€ä»»ä½•æ‰‹åŠ¨URLé…ç½®ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å‘ç°å¹¶è·å–æœ€æ–°çš„NHSæ•°æ®ï¼Œä¸ºç”¨æˆ·æä¾›å¯é çš„æ•°æ®å‚è€ƒæ¥æºã€‚ç®—æ³•å…·å¤‡å¼ºå¤§çš„é€‚åº”æ€§ï¼Œèƒ½å¤Ÿåº”å¯¹NHSç½‘ç«™çš„æœªæ¥å˜åŒ–ã€‚












NHS Waiting List Alert â€“ Native ETL Prototype

## 1. What is this?
This repository contains a minimal **non-Docker** proof-of-concept for ingesting NHS England *Referral to Treatment (RTT) provider-level* statistics, storing the data in PostgreSQL and visualising it with Grafana.  It implements the first two functional blocks (F1 Data Ingestion & F2 Data ETL) described in `nhs_alert_design.md`.

## 2. Repository layout
| Path | Purpose |
|------|---------|
| `bootstrap.py` | One-stop generator that writes **all** required project files (requirements, env template, ETL, PowerShell helpers, etc.) into a target directory and â€“ unless `--no-venv` â€“ also creates a local `.venv`, installs dependencies and downloads Playwright browsers. |
| `nhs_native/` | Stand-alone, non-Docker implementation of the ETL workflow. You should *mainly work here*. |
| `nhs_native/runner.py` | Core ETL script.  Discovers latest RTT-Provider CSV â†’ downloads â†’ normalises columns â†’ bulk-inserts into Postgres table `rtt_provider_raw` (idempotent via `ON CONFLICT DO NOTHING`). |
| `nhs_native/create_db.sql` | PostgreSQL DDL for database **nhs** and table `rtt_provider_raw`. |
| `nhs_native/requirements.txt` | Python dependencies required by the native ETL (Playwright, pandas, psycopg, requests). |
| PowerShell helpers | `run_etl_once.ps1`, `schedule_etl.ps1`, `start_grafana.ps1`. |
| `nhs_alert_design.md` | Product & technical design document â€“ long-term architecture, roadmap and business context. |

## 3. Quick start (Windows PowerShell)
```powershell
# 0) Clone repo and open PowerShell **as Administrator** (to allow Playwright browser install)
cd <your-working-dir>

# 1) Generate project files *and* virtualenv into a new folder (e.g. prod)
python bootstrap.py --path prod            # omit --no-venv to auto-install deps

# 2) Activate the fresh virtualenv
cd prod
. .\.venv\Scripts\Activate.ps1

# 3) Initialise PostgreSQL
#    3a) (first time) set password for role postgres â€“ see section 4.1 below
#    3b) create database & table schema
$env:PGPASSWORD = "secret"
psql -U postgres -h localhost -f create_db.sql

# 4) Run the ETL once (fetch â†’ load â†’ done)
python runner.py

# 5) Continuous mode (hourly, interval configurable via POLL_INTERVAL)
python runner.py --loop   # Ctrl+C to stop

# 6) (Optional) register daily Windows scheduled task
.\schedule_etl.ps1

# 7) (Optional) launch Grafana
.\start_grafana.ps1   # then open http://localhost:3000 (admin/admin)
```
The script also supports a continuous mode:
```powershell
python nhs_native\runner.py --loop   # repeats every hour
```

cd D:\testdata\nhs\prod_full
.\.venv\Scripts\Activate.ps1
$env:PGPASSWORD = "<ä½ çš„PGå¯†ç >"
psql -U postgres -h localhost -f create_db.sql   # åˆ›å»ºåº“è¡¨
python runner.py                                 # å•æ¬¡æŠ“å–å¹¶å†™å…¥
python runner.py --loop                          # æˆ–æŒç»­è¿è¡Œ

## 4. Environment variables
| Name | Example | Description |
|------|---------|-------------|
| `PG_CONN` | `postgresql://postgres:postgres@localhost:5432/nhs` | Postgres DSN used by the ETL runner. |
| `DATA_SOURCE` | `nhs` | Which fetcher in `data_sources.py` to use. Add your own fetchers and set this accordingly. |
| `POLL_INTERVAL` | `3600` | Seconds between ETL cycles in `--loop` mode. |
| `NHS_CSV_URL` | *(optional)* | Force a specific RTT-Provider CSV (bypasses discovery; only for the **nhs** data source). |

Variables can be exported in the shell or stored in a `.env` file copied from `env.example`. Tools like **direnv** or **python-dotenv** can auto-load them.

### 4.1 Setting or changing the Postgres password
A fresh Windows installation of PostgreSQL may have the default super-role `postgres` without a password. You can set (or reset) it with:
```powershell
# open psql shell (Windows run as admin / Mac & Linux adjust service control as needed)
psql -U postgres -h localhost

-- inside psql prompt       (terminate each line with semicolon)
ALTER USER postgres WITH PASSWORD 'postgres';
\q   -- quit
```
Alternatively you can create a dedicated user/db:
```sql
CREATE ROLE nhs_user WITH LOGIN PASSWORD 'secret';
CREATE DATABASE nhs OWNER nhs_user;
```
Then set `PG_CONN=postgresql://nhs_user:secret@localhost:5432/nhs`.

## 5. Data discovery logic
1. **Static scrape** â€“ Requests the RTT Waiting Times web page and uses a regex to locate links ending in `RTT-Provider.csv`.
2. **Dynamic scrape** â€“ If Playwright is available, launches headless Chromium to evaluate SPA-rendered HTML for additional links.
3. **Error handling** â€“ If no link is found, raises an explicit error instructing the operator to set `NHS_CSV_URL` manually.

## 6. Database schema
```sql
CREATE TABLE rtt_provider_raw(
    provider_code         TEXT,
    provider_name         TEXT,
    specialty_code        TEXT,
    specialty             TEXT,
    period                DATE,
    waiting_less_18_weeks INT,
    waiting_over_52_weeks INT,
    PRIMARY KEY (provider_code, specialty_code, period)
);
```
The ETL inserts via a temporary staging table and `ON CONFLICT DO NOTHING`, ensuring repeat runs are idempotent.

## 7. Roadmap (as per design doc)
* **Rule engine** â†’ JSON-configured thresholds per tenant.
* **Alert delivery** â†’ Push rich cards to WhatsUp / Teams / Slack webhooks.
* **Admin portal (FastAPI)** â†’ Manage rules, view alerts and health.
* **Multi-tenant & white-label** â†’ Separate schemas, CNAMEs & branding.

## 8. Testing strategy
* **Unit tests** â€“ Mock HTTP responses & Playwright to verify `latest_url()` resolution; patch Postgres with a test database to assert row counts and PK behaviour.
* **Integration tests** â€“ End-to-end run against a fixture CSV; monkey-patch `time.sleep` in loop mode to accelerate repeated inserts.
* **Resilience** â€“ Simulate network errors / 4xx responses and ensure the ETL logs the exception but continues in next loop.

A future PR will add a `tests/` directory with `pytest` suites implementing the above.

---
Â© 2025 AlertEdge Ltd â€“ internal prototype, not for production use. 